---
title: "No-Frills AI Product Search for a Well-Curated Catalog"
description: "Use an LLM to translate natural-language shopping queries into search DSL, preserving existing relevance and merchandising with lower latency and cost than vector-heavy RAG."
date: "Feb 2026"
publishedAt: "2026-02-12"
slug: "no-frills-ai-product-search-for-a-well-curated-catalog"
---

import { AIProductSearchDemo, BlogMarkdown } from '../../components'

```
> How can I implement conversational product search?
Searching 7 SaaS vendors...
```

Have you considered creating a new ETL pipeline which embeds and stores your data in a vector database? When a user asks a question, have an LLM extract or reconstruct the query. Then use an embedding model to embed it, run a vector search, and pipe the results back to your LLM. If the results are still meh, just throw in a specialized [rerank model](https://docs.cohere.com/docs/rerank). Just one more tool call, bro. I promise, just one more model and it'll fix everything. I mean, [what could go wrong](https://arxiv.org/html/2401.05856v1)? The results are still atrocious? Have you considered going [hybrid](https://techcommunity.microsoft.com/blog/azuredevcommunityblog/doing-rag-vector-search-is-not-enough/4161073) and using [RRF](https://cormack.uwaterloo.ca/cormacksigir09-rrf.pdf) for final-pass-ranking? Are you even clued in on the [theoretical limits of single vector RAG at Google scale](https://arxiv.org/html/2508.21038v1)?

```
3% context left
> /clear
```

```
> My product attributes are normalized — variant colors, dimensions, and other specs are mapped to canonical values — enabling faceted search (https://en.wikipedia.org/wiki/Faceted_search) and clean ETL into my search engine, which is already configured with relevance and merchandising rules.

> How can I implement conversational product search with optimal cost, latency and UX?

Thinking with ultra-extended-effort...
```

Get filter metadata across all product categories and cache it. A gaming laptop example:
```
categories.lvl0:Laptops|520
categories.lvl1:Laptops > Gaming Laptops|145
brands:Dell|95,HP|88,Lenovo|82,Apple|75,ASUS|68,Acer|52,MSI|35,Razer|25
prices.sale_price:[range]
attributes.screen_size:[range]
attributes.processor:Intel Core i7|120,Apple M3|75,AMD Ryzen 7|68,Intel Core i5|65,Apple M3 Pro|42,AMD Ryzen 9|38
attributes.ram:16GB|195,32GB|148,8GB|92,64GB|55
attributes.storage:512GB SSD|210,1TB SSD|175,256GB SSD|80,2TB SSD|55
attributes.color:Silver|165,Space Gray|98,Black|88,Midnight|45,Starlight|32
```

When the user’s query arrives and you’ve detected the category (via regex or an LLM), pull that category’s filter metadata.

If the user asks about "best gaming laptops under $1500 with a dark finish", the LLM has enough context to generate this Algolia payload:
```
{
  "index": "products_best_selling",
  "query": "",
  "facetFilters": [
    ["categories.lvl1:Laptops > Gaming Laptops"],
    ["attributes.color:Black", "attributes.color:Midnight"]
  ],
  "numericFilters": [
    "prices.sale_price <= 1500"
  ],
  "facets": [
    "brands",
    "attributes.processor",
    "attributes.ram",
    "attributes.storage",
    "attributes.color",
    "prices.sale_price",
    "attributes.screen_size"
  ]
}
```

How does an LLM do that?
1. It’s very good at constructing DSL syntax, in this case as an Algolia payload.
2. It has enough context (filter metadata) to determine which parts of the query map to a filter/value pair and which parts are numeric operators.
3. It can map semantic phrases to filter values when lexical matching fails: ‘dark finish’ becomes `Black OR Midnight`.

```
97% context left
> That's cool, I won't have to set up a new ETL or a vector database, nor will I need embedding or rerank model at runtime. A single fast LLM will get the job done and the context window won't have to include product data. But what UX does this unlock?
```

<AIProductSearchDemo />

The frontend receives a search payload that users can refine. When a user changes filters, the client issues a new query without another LLM roundtrip. Relevance and merchandising rules carry over from your search engine.


For follow-up messages, send the client's filter state and have the LLM adjust the payload:

<AIProductSearchDemo scenario="followup-core-i7" />

```
Press Ctrl-C again to exit
```

export default ({ children }) => <BlogMarkdown meta={meta}>{children}</BlogMarkdown>
