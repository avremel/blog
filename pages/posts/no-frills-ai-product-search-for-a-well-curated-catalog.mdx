---
title: "No-Frills AI Product Search for a Well-Curated Catalog"
description: "Short summary of this article"
date: "Feb 2026"
publishedAt: "2026-02-12"
slug: "no-frills-ai-product-search-for-a-well-curated-catalog"
---

import { AIProductSearchDemo, BlogMarkdown } from '../../components'

```
> How do I implement conversational product search?
Searching 23 devtool sites...
```

You want conversational product search? Have you considered an ETL pipeline which embeds and stores your data in a vector database? When a user asks a question, have an LLM extract or reconstruct the query. Then use an embedding model to embed it, run a vector search, and pipe the results back to your LLM. If the results are still meh, just throw in a specialized [rerank model](https://docs.cohere.com/docs/rerank). Just one more tool call, bro. I promise, just one more model and it'll fix everything. I mean, [what could go wrong](https://arxiv.org/html/2401.05856v1)? The results are still atrocious? Have you considered going [hybrid](https://techcommunity.microsoft.com/blog/azuredevcommunityblog/doing-rag-vector-search-is-not-enough/4161073) and using [RRF](https://cormack.uwaterloo.ca/cormacksigir09-rrf.pdf) for final-pass-ranking? Are you even clued in on the [theoretical limits of single vector RAG at Google scale](https://arxiv.org/html/2508.21038v1)?

```
0% context left
> /clear
```

```
> My product data is structured and standardized to enable faceted search (https://en.wikipedia.org/wiki/Faceted_search). My search engine is already configured with my relevance and merchandising rules.

> How can I implement conversational product search with optimal cost, latency and UX?
```

Get filter metadata across all product categories and cache it. A gaming laptop example:
```
categories.lvl0:Laptops|520
categories.lvl1:Laptops > Gaming Laptops|145
brands:Dell|95,HP|88,Lenovo|82,Apple|75,ASUS|68,Acer|52,MSI|35,Razer|25
prices.sale_price:[range]
attributes.screen_size:[range]
attributes.processor:Intel Core i7|120,Apple M3|75,AMD Ryzen 7|68,Intel Core i5|65,Apple M3 Pro|42,AMD Ryzen 9|38
attributes.ram:16GB|195,32GB|148,8GB|92,64GB|55
attributes.storage:512GB SSD|210,1TB SSD|175,256GB SSD|80,2TB SSD|55
attributes.color:Silver|165,Space Gray|98,Black|88,Midnight|45,Starlight|32
```

When the user query comes in and you have detected the category (using `regex` or an LLM), pull in its filter metadata.

If the user asks about "Gaming laptops under $1500 with a dark finish", the LLM has enough context to generate this Algolia payload:
```
{
  "query": "",
  "facetFilters": [
    ["categories.lvl1:Laptops > Gaming Laptops"],
    ["attributes.color:Black", "attributes.color:Midnight"]
  ],
  "numericFilters": [
    "prices.sale_price <= 1500"
  ],
  "facets": [
    "brands",
    "attributes.processor",
    "attributes.ram",
    "attributes.storage",
    "attributes.color",
    "prices.sale_price",
    "attributes.screen_size"
  ]
}
```

How does an LLM do that?
1. It is very good at constructing DSL (domain specific language) syntax, in this case an Algolia payload.
2. It has enough context (filter metadata) to determine which parts of the query map to a filter/value pair and which parts are numeric operators.
3. It can a semantic mapping of terms to filter values where lexical search fails: "dark finish" becomes `Black OR Midnight`.

```
97% context left
> What UX does this unlock?
```

<AIProductSearchDemo />

export default ({ children }) => <BlogMarkdown meta={meta}>{children}</BlogMarkdown>
